{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Next_Word_Prediction_using_Universal_Sentence_Encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryanh8/NextWordPredictor/blob/main/Next_Word_Prediction_using_Universal_Sentence_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zavljtkc2Xsv"
      },
      "source": [
        "# Use Google Pre-trained Universal Sentences Encoder to train a NLP Model\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_qUksc3r3KX"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d9_NB3vpwG8"
      },
      "source": [
        "# Getting all required libraries\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "import gdown\r\n",
        "import numpy\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import tensorflow as tf\r\n",
        "from absl import logging\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow import keras\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.models import Sequential\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from keras.layers.recurrent import LSTM\r\n",
        "from keras.layers import Dense, Activation\r\n",
        "from keras.callbacks import LambdaCallback\r\n",
        "from keras.utils.data_utils import get_file\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHNTF6AHsQUL"
      },
      "source": [
        "## **Data preparation - _Generating Corpus_**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B76KQMiJCN9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f010eab6-3eea-4fef-a6e1-1b3694324e08"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt -O corpus.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-06 16:18:55--  https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7540200 (7.2M) [text/plain]\n",
            "Saving to: ‘corpus.txt’\n",
            "\n",
            "corpus.txt          100%[===================>]   7.19M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-02-06 16:18:55 (112 MB/s) - ‘corpus.txt’ saved [7540200/7540200]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb2U0kS38MRf"
      },
      "source": [
        "# Read local file from directory\r\n",
        "with open('corpus.txt') as subject:\r\n",
        "  cache = subject.readlines()\r\n",
        "translator = str.maketrans('', '', string.punctuation) # Remove punctuation\r\n",
        "lines = [doc.lower().translate(translator) for doc in cache] # Switch to lower case"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMvgmMBEF7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a101efa-c039-4864-a34d-1ea15b20a9aa"
      },
      "source": [
        "# PREVIEW OUTPUT ::\r\n",
        "\r\n",
        "print(lines[0][:100])\r\n",
        "len(lines)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in science and engineering intelligent processing of complex signals such as images sound or languag\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKFp-u8951-g"
      },
      "source": [
        "# Generate an list of single/independent words\r\n",
        "\r\n",
        "vocabulary = list(set(' '.join(lines).replace('\\n','').split(' ')))\r\n",
        "primary_store = {}\r\n",
        "for strings, texts in enumerate(vocabulary):\r\n",
        "  primary_store[texts] = strings"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7pOLvIhEZZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac55dffa-c63e-4a16-cf20-0921fdfd7622"
      },
      "source": [
        "# PREVIEW OUTPUT ::\r\n",
        "\r\n",
        "print(vocabulary[:50])\r\n",
        "len(vocabulary)\r\n",
        "print(primary_store)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'handwriting', 'em', 'allowing', 'computationally', 'renewed', 'obtained', 'opportunities', 'sometimes', 'phase', 'spark', 'big', 'about', 'generates', 'replaces', 'subnetworks', 'mediated', 'hmm', 'worlds', 'heuristic', 'suboptimal', 'yielded', 'systems', 'explicitly', 'irregular', 'thought', 'demonstrate', 'terms', 'rprops', 'treatment', 'university', 'twodimensions', 'sizes', 'rigorous', 'prove', 'patterns', 'rare', 'these', 'tested', 'phoneme', 'transitions', 'strided', 'chosen', 'undirected', 'learned', 'finegrained', 'confidentinformationfirst', 'complex', 'arbitrary', 'coding']\n",
            "{'': 0, 'handwriting': 1, 'em': 2, 'allowing': 3, 'computationally': 4, 'renewed': 5, 'obtained': 6, 'opportunities': 7, 'sometimes': 8, 'phase': 9, 'spark': 10, 'big': 11, 'about': 12, 'generates': 13, 'replaces': 14, 'subnetworks': 15, 'mediated': 16, 'hmm': 17, 'worlds': 18, 'heuristic': 19, 'suboptimal': 20, 'yielded': 21, 'systems': 22, 'explicitly': 23, 'irregular': 24, 'thought': 25, 'demonstrate': 26, 'terms': 27, 'rprops': 28, 'treatment': 29, 'university': 30, 'twodimensions': 31, 'sizes': 32, 'rigorous': 33, 'prove': 34, 'patterns': 35, 'rare': 36, 'these': 37, 'tested': 38, 'phoneme': 39, 'transitions': 40, 'strided': 41, 'chosen': 42, 'undirected': 43, 'learned': 44, 'finegrained': 45, 'confidentinformationfirst': 46, 'complex': 47, 'arbitrary': 48, 'coding': 49, 'grand': 50, 'third': 51, 'nets': 52, 'width': 53, 'poor': 54, 'clear': 55, 'abstract': 56, 'classindependent': 57, 'store': 58, 'consequently': 59, 'mathematically': 60, 'curved': 61, 'near': 62, 'stable': 63, 'reduce': 64, 'light': 65, 'computing': 66, 'quickly': 67, 'handwritten': 68, 'must': 69, 'algorithms': 70, 'reinforcement': 71, 'further': 72, 'structurebased': 73, 'too': 74, 'developments': 75, 'optimise': 76, 'comparison': 77, 'i': 78, 'depends': 79, 'dramatic': 80, 'relationship': 81, 'so': 82, 'style': 83, 'distances': 84, 'pursue': 85, 'never': 86, 'predict': 87, 'ancestral': 88, 'predictions': 89, 'emerged': 90, 'together': 91, 'supporting': 92, 'cost': 93, 'database': 94, 'higher': 95, 'better': 96, 'improper': 97, 'due': 98, 'as': 99, 'present': 100, 'initializers': 101, 'gametheoretic': 102, 'universal': 103, '23x': 104, 'lowprecision': 105, 'implications': 106, 'roughly': 107, 'lp': 108, 'doesnt': 109, 'layerwise': 110, 'optimizers': 111, 'gasf': 112, 'compare': 113, 'incur': 114, 'masking': 115, 'locallyconnected': 116, 'nowubiquitous': 117, 'preconditioning': 118, 'four': 119, 'other': 120, 'addressed': 121, 'qualitatively': 122, 'gpu': 123, 'report': 124, 'portions': 125, 'benign': 126, 'highly': 127, 'digit': 128, 'fully': 129, 'vulnerability': 130, 'establishes': 131, 'overcome': 132, 'relatively': 133, 'per': 134, 'l2': 135, 'initial': 136, 'consider': 137, 'gramian': 138, 'apply': 139, 'suffer': 140, '128processor': 141, 'weakly': 142, 'decoded': 143, 'discriminate': 144, 'storage': 145, 'recognized': 146, 'erm': 147, 'only': 148, '13': 149, 'pairwise': 150, 'utilized': 151, 'physics': 152, 'classes': 153, 'estimate': 154, 'multiplicative': 155, 'inverse': 156, 'see': 157, 'spectrogram': 158, 'provable': 159, 'convolution': 160, 'centers': 161, 'regions': 162, 'unfolding': 163, 'set': 164, 'vector': 165, 'ones': 166, 'interpretable': 167, 'structured': 168, 'restricted': 169, 'prior': 170, 'point': 171, 'propagated': 172, 'mtf': 173, 'visiting': 174, 'simple': 175, 'minibatching': 176, 'fidelity': 177, 'elementwise': 178, 'expected': 179, 'active': 180, 'maxnorm': 181, 'dropconnect': 182, 'highnuisance': 183, 'applications': 184, 'updating': 185, 'insight': 186, 'their': 187, 'standpoint': 188, 'hope': 189, 'corresponding': 190, 'currently': 191, 'learnable': 192, 'pharmaceutical': 193, 'a': 194, 'normalization': 195, 'lead': 196, 'coordinates': 197, 'family': 198, 'causing': 199, 'intuition': 200, 'limiting': 201, 'interface': 202, 'rise': 203, 'account': 204, 'cifar10': 205, 'resulting': 206, 'gain': 207, 'defensive': 208, 'preprocessing': 209, 'overhead': 210, 'parametrically': 211, 'original': 212, 'product': 213, 'derive': 214, 'maxout': 215, 'next': 216, 'confident': 217, 'pieces': 218, 'residual': 219, 'descriptor': 220, 'efficiently': 221, 'incorporates': 222, 'real': 223, 'matches': 224, 'neither': 225, 'rescaling': 226, 'shift': 227, 'like': 228, 'caused': 229, 'studies': 230, 'establish': 231, 'companion': 232, 'bayesian': 233, 'straightforward': 234, 'hessian': 235, 'customer': 236, 'corresponds': 237, 'final': 238, 'consume': 239, 'above': 240, 'therefore': 241, 'largescale': 242, 'likely': 243, 'updates': 244, 'perfect': 245, 'spaces': 246, 'clustering': 247, 'select': 248, 'datas': 249, 'variant': 250, 'shuffling': 251, 'geometry': 252, 'microprocessors': 253, 'speaker': 254, 'subspace': 255, 'variance': 256, 'asynchronous': 257, 'formal': 258, 'explains': 259, 'solving': 260, 'statistics': 261, 'subsets': 262, 'maximize': 263, 'answer': 264, 'through': 265, 'boundaries': 266, 'growing': 267, 'identifies': 268, 'assertion': 269, 'use': 270, 'additive': 271, 'encode': 272, 'originally': 273, 'late': 274, 'until': 275, 'rates': 276, 'aggregating': 277, 'metrics': 278, 'reconstruction': 279, 'multilinear': 280, 'points': 281, 'explored': 282, 'successive': 283, 'compressed': 284, 'gating': 285, 'encoded': 286, 'reidentification': 287, 'show': 288, 'seen': 289, 'group': 290, 'atomnet': 291, 'alternating': 292, 'automatic': 293, '1996': 294, 'selection': 295, 'significantly': 296, 'discuss': 297, 'sensitivity': 298, 'assertions': 299, 'reusing': 300, 'benefit': 301, 'wer': 302, 'shown': 303, 'coarsegraining': 304, 'sampling': 305, 'impressive': 306, 'languages': 307, 'gives': 308, 'components': 309, 'content': 310, 'pass': 311, 'sparknet': 312, 'rir': 313, 'notice': 314, 'data': 315, 'frontend': 316, 'which': 317, 'connections': 318, 'there': 319, 'consequences': 320, 'exponential': 321, 'nondegeneracy': 322, 'authentication': 323, 'levels': 324, 'rprop': 325, 'lengths': 326, 'rglike': 327, 'observe': 328, 'containing': 329, 'variation': 330, 'transposed': 331, 'extra': 332, 'uninformative': 333, 'unit': 334, 'available': 335, 'avoids': 336, 'overlap': 337, 'conclusions': 338, 'shed': 339, 'linearly': 340, 'optimization': 341, 'packages': 342, 'define': 343, 'nuisance': 344, 'transfer': 345, 'kind': 346, 'intensive': 347, 'indeed': 348, 'directions': 349, 'rigorously': 350, 'perspective': 351, 'summationdifference': 352, 'maximal': 353, 'developed': 354, 'neuronal': 355, 'deviatoractorcritic': 356, 'plays': 357, 'way': 358, 'weights': 359, 'effect': 360, 'examples': 361, 'consistently': 362, 'language': 363, 'methodologies': 364, 'compositional': 365, 'analyses': 366, 'cluster': 367, 'achieve': 368, 'poses': 369, 'mean': 370, 'theory': 371, 'performance': 372, 'despite': 373, 'parameters': 374, 'incorporate': 375, 'pathsgd': 376, 'internal': 377, 'synaptic': 378, 'scalenormalization': 379, 'perceptrons': 380, 'drop': 381, '90': 382, 'presence': 383, 'cd': 384, 'still': 385, 'neighbourhood': 386, 'timeconsuming': 387, 'normconstrained': 388, 'overfitting': 389, 'dsn': 390, 'cifar100': 391, 'constructed': 392, 'world': 393, 'fractions': 394, 'synchrony': 395, 'communication': 396, 'mixture': 397, 'metric': 398, 'contribution': 399, 'traditional': 400, 'shortterm': 401, 'octopus': 402, 'architecture': 403, 'independent': 404, 'prediction': 405, 'structureeg': 406, 'property': 407, 'kcenters': 408, 'endtoend': 409, 'billions': 410, 'dependencies': 411, 'utilizing': 412, 'sensing': 413, 'cp': 414, 'resilient': 415, 'into': 416, 'lot': 417, 'believed': 418, 'phonetic': 419, 'different': 420, 'recover': 421, 'modification': 422, 'ffns': 423, 'formally': 424, 'inputoutput': 425, 'simulation': 426, 'valuefunction': 427, 'explore': 428, 'smooth': 429, 'properties': 430, 'particularly': 431, 'exponentially': 432, 'arm': 433, 'depthdependency': 434, 'or': 435, 'effectively': 436, 'transform': 437, 'fewer': 438, '50hour': 439, 'streams': 440, 'initialize': 441, 'separating': 442, 'variety': 443, 'significant': 444, 'common': 445, 'shaped': 446, 'spurious': 447, 'allowed': 448, 'graphical': 449, 'backpropagationstyle': 450, 'extract': 451, 'presents': 452, 'why': 453, 'depend': 454, 'pretraining': 455, 'infer': 456, 'svrg': 457, 'parameterized': 458, 'maintains': 459, 'believe': 460, 'reduction': 461, 'acts': 462, 'classified': 463, 'functional': 464, 'serve': 465, 'limitations': 466, 'jacobian': 467, 'divided': 468, 'consists': 469, 'promising': 470, 'connectionist': 471, 'ever': 472, 'demonstrating': 473, 'easy': 474, 'consuming': 475, 'variational': 476, 'mathematical': 477, 'matrices': 478, 'capabilities': 479, 'node': 480, 'modify': 481, 'optimum': 482, 'illegal': 483, 'formulations': 484, 'mappings': 485, 'flow': 486, 'times': 487, 'producing': 488, 'procedures': 489, 'game': 490, 'guide': 491, 'favorable': 492, 'learners': 493, 'parameterization': 494, 'we': 495, 'easier': 496, 'square': 497, 'entire': 498, 'operates': 499, 'rbms': 500, 'invariant': 501, 'around': 502, 'speedups': 503, 'receives': 504, 'face': 505, 'every': 506, 'formulated': 507, '300hr': 508, 'transformed': 509, 'bn': 510, '1992': 511, 'approximate': 512, 'angular': 513, 'cramerrao': 514, 'pressure': 515, 'meaningful': 516, 'protocols': 517, 'biologically': 518, 'configuration': 519, 'discrete': 520, 'yield': 521, 'deeplysupervised': 522, 'gaussian': 523, 'are': 524, 'mac': 525, 'modifying': 526, 'topology': 527, 'up': 528, 'modulators': 529, 'high': 530, 'scaling': 531, 'composing': 532, 'compensate': 533, 'our': 534, 'incorporated': 535, 'nearly': 536, 'tucker': 537, 'broadcast': 538, 'basis': 539, 'nature': 540, 'indicating': 541, 'maximally': 542, '454': 543, 'speedup': 544, 'environments': 545, 'followed': 546, 'underlying': 547, 'higherorder': 548, 'normal': 549, 'nmfs': 550, 'labeling': 551, 'revisit': 552, 'understood': 553, 'take': 554, 'decreases': 555, 'seem': 556, 'textitdeep': 557, 'retain': 558, 'complicated': 559, 'ista': 560, 'variable': 561, 'exactly': 562, 'emerge': 563, 'during': 564, 'tensor': 565, 'imagenet': 566, 'precisely': 567, 'rnns': 568, 'build': 569, 'combine': 570, 'remained': 571, 'embedding': 572, 'outlining': 573, 'padding': 574, 'amount': 575, 'experiment': 576, 'document': 577, 'designing': 578, 'examined': 579, 'unable': 580, 'scalepreserving': 581, 'impact': 582, 'determinant': 583, 'gradients': 584, 'brand': 585, 'perturbations': 586, 'weaker': 587, 'remaining': 588, 'wide': 589, 'originate': 590, 'systematic': 591, 'elements': 592, 'demands': 593, 'yet': 594, 'minibatches': 595, 'discussed': 596, 'visualization': 597, 'force': 598, 'symbolic': 599, 'assigned': 600, 'justifies': 601, 'savings': 602, 'advantageous': 603, 'pathway': 604, 'made': 605, 'missed': 606, 'counterparts': 607, 'illustrated': 608, 'same': 609, 'boson': 610, 'compact': 611, 'responsible': 612, 'preserves': 613, 'posteriori': 614, 'recursive': 615, 'dropouts': 616, 'each': 617, 'could': 618, 'maps': 619, 'root': 620, 'twospeaker': 621, 'parmac': 622, 'reuse': 623, 'hypotheses': 624, 'lowrank': 625, 'members': 626, 'improve': 627, 'successfully': 628, 'looked': 629, 'great': 630, 'multiplications': 631, 'clusterings': 632, 'firstly': 633, 'discriminativeness': 634, 'highlatency': 635, 'reducing': 636, 'maxpooling': 637, 'do': 638, 'empirical': 639, 'intertwined': 640, 'deterministic': 641, 'map': 642, 'resources': 643, 'depth': 644, 'onto': 645, 'connected': 646, 'resnet': 647, 'emits': 648, 'emergence': 649, 'server': 650, 'assumes': 651, 'discriminatively': 652, 'size': 653, 'intrinsic': 654, 'science': 655, 'thus': 656, 'studied': 657, 'elusive': 658, 'address': 659, 'input': 660, 'stream': 661, 'rely': 662, '1': 663, 'extends': 664, 'no': 665, 'leverages': 666, 'examine': 667, 'efficiency': 668, 'increase': 669, 'description': 670, 'index': 671, '751': 672, 'convey': 673, 'appropriate': 674, 'chip': 675, 'create': 676, 'explain': 677, 'evaluated': 678, 'involves': 679, 'regarded': 680, 'enforces': 681, 'none': 682, 'program': 683, 'cif': 684, 'approach': 685, 'justifications': 686, 'generating': 687, 'accelerate': 688, 'reliably': 689, 'nmf': 690, 'upper': 691, 'describe': 692, 'modes': 693, 'fusion': 694, 'compressive': 695, 'biometric': 696, 'backpropagation': 697, 'defenses': 698, 'differences': 699, 'aims': 700, 'obstacles': 701, 'configurations': 702, 'word': 703, '2012': 704, 'equivalent': 705, 'previous': 706, 'time': 707, 'deeply': 708, 'reduces': 709, 'remarkable': 710, 'between': 711, 'rg': 712, 'text': 713, 'controlled': 714, 'probabilities': 715, 'prominent': 716, 'decoding': 717, 'interference': 718, 'probe': 719, 'tanh': 720, 'tuning': 721, 'speed': 722, 'have': 723, 'approximation': 724, 'dropping': 725, 'below': 726, 'contrastive': 727, 'exploits': 728, 'conducted': 729, 'occasionally': 730, 'methodology': 731, 'enables': 732, 'gpus': 733, 'human': 734, 'steps': 735, 'piecewise': 736, 'compress': 737, 'music': 738, 'perceptual': 739, 'statistical': 740, 'explanation': 741, 'comparing': 742, 'obliterate': 743, 'guard': 744, 'largest': 745, 'work': 746, 'scale': 747, 'correct': 748, 'simply': 749, 'trainable': 750, 'fullyconnected': 751, 'partial': 752, 'grow': 753, 'things': 754, 'fisher': 755, 'optimizing': 756, 'flowing': 757, 'obtain': 758, 'stl10': 759, 'units': 760, '23': 761, 'workloads': 762, 'being': 763, 'structures': 764, 'here': 765, 'developing': 766, 'illicit': 767, 'need': 768, 'decay': 769, 'optimisation': 770, 'preexisting': 771, 'curvature': 772, '800': 773, 'possibly': 774, 'activity': 775, 'parametrization': 776, 'derived': 777, 'experiments': 778, 'emphuntie': 779, 'conjunction': 780, 'turning': 781, 'marginalized': 782, 'improving': 783, 'advances': 784, 'observations': 785, 'explained': 786, 'nphard': 787, 'attacks': 788, 'conversational': 789, 'generators': 790, 'fulfills': 791, 'geometrically': 792, 'generalized': 793, 'abstractions': 794, 'willing': 795, 'and': 796, 'numerical': 797, 'communicationintensive': 798, 'batch': 799, 'additionally': 800, 'corollaries': 801, 'boltzmann': 802, 'generative': 803, 'formalize': 804, 'promise': 805, 'deliver': 806, 'design': 807, 'aim': 808, 'confirmed': 809, 'full': 810, '05': 811, 'class': 812, 'incorporating': 813, 'protocol': 814, 'privacy': 815, 'sgd': 816, 'el': 817, 'endowing': 818, 'straightthrough': 819, 'blstm': 820, 'frame': 821, 'separation': 822, 'edge': 823, 'desired': 824, 'distance': 825, 'retaining': 826, 'mixed': 827, 'question': 828, 'exploration': 829, 'past': 830, 'benefited': 831, 'long': 832, 'difficult': 833, 'history': 834, 'decide': 835, 'hardness': 836, 'later': 837, 'representation': 838, 'scala': 839, 'help': 840, 'computable': 841, 'innovations': 842, 'intelligent': 843, 'states': 844, 'sources': 845, 'representing': 846, 'indiscriminate': 847, '09': 848, 'technology': 849, 'calculating': 850, 'respectively': 851, 'effective': 852, 'straight': 853, 'inference': 854, 'convnets': 855, 'natural': 856, 'dualstream': 857, 'impossible': 858, 'longterm': 859, 'bits': 860, 'will': 861, 'gains': 862, 'learn': 863, 'position': 864, 'yields': 865, 'brain': 866, 'frequent': 867, 'prefix': 868, 'nonnegative': 869, 'opening': 870, 'riskaverting': 871, 'million': 872, 'characterized': 873, 'known': 874, 'telephone': 875, 'computes': 876, 'limit': 877, 'defined': 878, 'lpnorm': 879, 'sets': 880, 'insidecnn': 881, '12184802': 882, 'grammars': 883, 'instead': 884, 'timit': 885, 'stability': 886, 'user': 887, 'control': 888, 'respect': 889, 'little': 890, 'achieves': 891, 'across': 892, 'backbone': 893, 'character': 894, 'local': 895, 'interaction': 896, 'boost': 897, 'transformations': 898, 'defines': 899, 'supported': 900, 'minimizes': 901, 'reveals': 902, 'dictionary': 903, 'rate': 904, 'news': 905, 'produce': 906, 'according': 907, 'while': 908, 'creation': 909, 'mapping': 910, 'focusing': 911, 'recently': 912, 'minimizer': 913, 'dataset': 914, 'array': 915, 'decomposes': 916, 'paris': 917, '30000x': 918, 'paths': 919, 'attempt': 920, 'practice': 921, 'tolerance': 922, 'temporaldifference': 923, 'mse': 924, 'projection': 925, 'depth4': 926, 'intra': 927, 'imperfections': 928, 'rescaled': 929, 'symbol': 930, 'correctness': 931, 'datacenterscale': 932, '15x': 933, '01': 934, 'analyzing': 935, 'speeding': 936, 'powerful': 937, 'suggesting': 938, 'giant': 939, 'propose': 940, 'machinery': 941, 'led': 942, 'toronto': 943, 'coordinate': 944, 'can': 945, 'adam': 946, 'minibatch': 947, 'been': 948, 'replace': 949, 'standalone': 950, 'cdcif': 951, 'happens': 952, 'probabilistic': 953, 'because': 954, 'frameworks': 955, 'elected': 956, 'works': 957, 'predicted': 958, 'technique': 959, 'hierarchically': 960, 'approaches': 961, 'spectral': 962, 'lvcsr': 963, 'vanishing': 964, 'discussing': 965, 'practically': 966, 'policy': 967, 'assessed': 968, 'zeroshot': 969, 'minima': 970, 'advantages': 971, 'chen': 972, 'of': 973, 'layertolayer': 974, 'memorizing': 975, 'rectifiers': 976, 'categorization': 977, 'unreliable': 978, 'accesses': 979, 'retained': 980, 'footprint': 981, 'perception': 982, 'target': 983, 'cumbersome': 984, 'molecules': 985, 'either': 986, 'universite': 987, 'elaborating': 988, 'multiplication': 989, 'contrast': 990, 'network': 991, 'automatically': 992, 'operation': 993, 'employing': 994, 'implement': 995, 'overcoming': 996, 'stationary': 997, 'der': 998, 'recognizing': 999, 'socher': 1000, 'develop': 1001, 'chemical': 1002, 'decreasing': 1003, 'labeled': 1004, 'approximators': 1005, 'back': 1006, 'competition': 1007, 'graph': 1008, 'computations': 1009, 'games': 1010, 'date': 1011, 'effort': 1012, 'transforming': 1013, 'unbiased': 1014, 'approximating': 1015, 'leads': 1016, 'ask': 1017, 'recognition': 1018, 'rectified': 1019, 'provide': 1020, 'describing': 1021, 'extensions': 1022, 'estimator': 1023, 'surprise': 1024, 'affinity': 1025, 'ingredients': 1026, 'child': 1027, 'pronunciation': 1028, 'serving': 1029, 'learns': 1030, 'specialize': 1031, 'transparent': 1032, 'improved': 1033, 'eigenvectors': 1034, 'relationships': 1035, 'reading': 1036, 'neocognitrons': 1037, 'arbitrarily': 1038, 'postsynaptic': 1039, '6': 1040, 'uses': 1041, 'gatedregret': 1042, 'utilizes': 1043, 'simulated': 1044, 'richer': 1045, 'synchronization': 1046, 'logspace': 1047, '84000': 1048, '8': 1049, 'layer': 1050, 'notion': 1051, 'tc': 1052, 'details': 1053, 'preserving': 1054, 'performs': 1055, 'generalize': 1056, 'parameter': 1057, 'make': 1058, 'part': 1059, 'solved': 1060, 'supports': 1061, 'critically': 1062, 'preserve': 1063, 'system': 1064, 'behind': 1065, 'simultaneously': 1066, 'vectors': 1067, 'manipulate': 1068, 'semantics': 1069, 'analytics': 1070, 'gradient': 1071, 'improves': 1072, 'removing': 1073, 'explicit': 1074, 'contextual': 1075, 'instance': 1076, 'intuitive': 1077, 'some': 1078, 'ideas': 1079, 'trees': 1080, 'vehicles': 1081, 'realistic': 1082, 'gatedfeedback': 1083, 'reason': 1084, 'proof': 1085, 'hinton': 1086, 'novel': 1087, 'feedforward': 1088, 'days': 1089, 'computer': 1090, 'evaluation': 1091, 'retrained': 1092, '0743': 1093, 'verify': 1094, 'roles': 1095, 'combined': 1096, 'stacked': 1097, 'modulation': 1098, 'exploding': 1099, 'monro': 1100, 'staleness': 1101, 'track': 1102, 'carefully': 1103, 'targetdomain': 1104, 'glm': 1105, 'limited': 1106, 'typically': 1107, 'order': 1108, 'free': 1109, 'algorithmic': 1110, 'expressive': 1111, 'inputtohidden': 1112, 'kernel': 1113, 'segmentation': 1114, 'hashing': 1115, 'commercial': 1116, 'transition': 1117, 'source': 1118, 'nonconvex': 1119, 'restriction': 1120, 'decoder': 1121, 'hence': 1122, 'requires': 1123, 'required': 1124, 'equivalence': 1125, 'tremendously': 1126, 'compatible': 1127, 'covariance': 1128, 'potential': 1129, 'generalizability': 1130, 'access': 1131, 'accelerators': 1132, 'namely': 1133, 'give': 1134, 'dac': 1135, '2': 1136, 'behaviour': 1137, 'increasing': 1138, 'nearestneighbor': 1139, 'claims': 1140, 'concepts': 1141, 'initializations': 1142, 'preservation': 1143, 'among': 1144, 'estimators': 1145, 'repeat': 1146, 'achieving': 1147, 'trillions': 1148, 'successful': 1149, 'impractical': 1150, 'goodfellow': 1151, 'mild': 1152, 'loglikelihood': 1153, 'benchmark': 1154, 'tcdnnblstmdnn': 1155, 'controlling': 1156, '100': 1157, 'moments': 1158, 'embedded': 1159, 'multilayer': 1160, 'orbits': 1161, 'determine': 1162, 'dimensionality': 1163, 'microphone': 1164, 'function': 1165, 'multivariate': 1166, 'pathways': 1167, 'hf': 1168, 'iterations': 1169, 'get': 1170, 'flattens': 1171, 'probability': 1172, 'towards': 1173, 'preliminary': 1174, 'lies': 1175, 'energyefficient': 1176, 'represented': 1177, 'likelihood': 1178, 'sum': 1179, 'hmax': 1180, 'average': 1181, 'pathwise': 1182, 'popular': 1183, 'security': 1184, 'path': 1185, 'on': 1186, 'supervision': 1187, 'codes': 1188, 'converge': 1189, 'values': 1190, 'two': 1191, 'winning': 1192, 'building': 1193, 'steepest': 1194, 'factorization': 1195, 'caruana': 1196, 'adopted': 1197, 'machine': 1198, 'grows': 1199, 'devastating': 1200, 'detection': 1201, 'wall': 1202, 'includes': 1203, 'cudasupport': 1204, 'proposes': 1205, 'decoders': 1206, 'often': 1207, 'bandit': 1208, 'metadata': 1209, 'firing': 1210, 'practical': 1211, 'constrained': 1212, 'state': 1213, 'segmentations': 1214, 'tree': 1215, 'include': 1216, 'trick': 1217, 'fault': 1218, 'welltrained': 1219, 'minimizers': 1220, 'consequence': 1221, 'strategy': 1222, 'affine': 1223, 'blocks': 1224, 'lbfgs': 1225, 'feature': 1226, 'starting': 1227, '988': 1228, 'sequentially': 1229, 'estimation': 1230, 'chain': 1231, 'interpret': 1232, 'lstm': 1233, 'events': 1234, 'outofclass': 1235, 'equally': 1236, 'constructs': 1237, 'label': 1238, 'portion': 1239, 'batchprocessing': 1240, 'balancing': 1241, 'circumvent': 1242, 'reaching': 1243, 'score': 1244, 'reaches': 1245, 'rdds': 1246, 'superior': 1247, 'construct': 1248, 'highest': 1249, 'distinct': 1250, 'unfortunately': 1251, 'autoencoders': 1252, 'learner': 1253, 'provided': 1254, 'multiplying': 1255, 'acceleration': 1256, 'tools': 1257, 'atomnets': 1258, 'small': 1259, 'claim': 1260, 'composition': 1261, 'becomes': 1262, 'pipelines': 1263, 'very': 1264, 'treestructured': 1265, 'latest': 1266, 'proportional': 1267, 'more': 1268, 'rank1': 1269, 'churned': 1270, 'then': 1271, 'operations': 1272, 'thoroughly': 1273, 'remain': 1274, 'drastically': 1275, 'multimodal': 1276, 'capacity': 1277, 'interpretations': 1278, 'dealing': 1279, 'modeling': 1280, 'calculate': 1281, 'existence': 1282, 'region': 1283, 'earlier': 1284, 'hihi': 1285, 'formed': 1286, 'correspond': 1287, 'randomized': 1288, 'total': 1289, 'costof': 1290, 'outcomes': 1291, 'lateral': 1292, 'principle': 1293, 'computers': 1294, 'realize': 1295, 'commonly': 1296, 'demand': 1297, 'teams': 1298, 'retrieval': 1299, 'infinite': 1300, 'taken': 1301, 'discovered': 1302, 'changes': 1303, 'wrt': 1304, 'image': 1305, 'variations': 1306, 'comprises': 1307, 'employs': 1308, 'conditions': 1309, 'convolutions': 1310, 'now': 1311, 'taking': 1312, 'flexible': 1313, 'x': 1314, 'desirable': 1315, 'since': 1316, 'structural': 1317, 'hierarchical': 1318, 'auxiliary': 1319, 'various': 1320, 'robust': 1321, 'well': 1322, 'rule': 1323, 'added': 1324, 'sparser': 1325, 'rewritten': 1326, 'generalization': 1327, 'invariance': 1328, 'gradually': 1329, 'bengio': 1330, 'requiring': 1331, 'investigation': 1332, 'lowerbounded': 1333, 'pooling': 1334, 'inducing': 1335, 'large': 1336, 'flip': 1337, 'monotonic': 1338, 'trains': 1339, 'rootmeansquare': 1340, 'side': 1341, 'generic': 1342, 'pca': 1343, 'showing': 1344, 'mixtures': 1345, 'px': 1346, 'suggests': 1347, 'its': 1348, 'means': 1349, 'samples': 1350, 'implicitly': 1351, 'net': 1352, 'sophisticated': 1353, 'necessary': 1354, '412': 1355, 'unseen': 1356, 'associated': 1357, 'compared': 1358, 'poorly': 1359, 'where': 1360, 'crucial': 1361, 'both': 1362, 'corpus': 1363, 'rbm': 1364, 'basic': 1365, 'generalizes': 1366, 'importance': 1367, 'exclusively': 1368, 'encounters': 1369, 'fields': 1370, 'come': 1371, 'remains': 1372, 'represent': 1373, 'moreover': 1374, 'extraction': 1375, 'black': 1376, 'lying': 1377, 'to': 1378, 'descriptive': 1379, 'practitioners': 1380, 'backgrounds': 1381, 'quality': 1382, 'customers': 1383, 'persistent': 1384, 'expense': 1385, 'existing': 1386, 'shape': 1387, 'sized': 1388, 'parent': 1389, 'polynomial': 1390, 'algorithm': 1391, 'the': 1392, 'seminonnegative': 1393, 'vectorized': 1394, 'value': 1395, 'log': 1396, 'should': 1397, 'scaled': 1398, 'activities': 1399, 'enough': 1400, 'operating': 1401, 'frequently': 1402, 'good': 1403, 'products': 1404, 'stages': 1405, 'simnets': 1406, '5': 1407, 'generalizing': 1408, 'adaption': 1409, 'wellknown': 1410, 'groups': 1411, 'manifolds': 1412, 'upon': 1413, 'areas': 1414, 'study': 1415, 'approximately': 1416, 'unsupervised': 1417, 'estimating': 1418, 'convexity': 1419, 'fairly': 1420, 'step': 1421, 'quantitatively': 1422, 'mentioned': 1423, 'sound': 1424, 'norm': 1425, 'detected': 1426, 'created': 1427, 'subset': 1428, 'denoising': 1429, 'know': 1430, 'hebbian': 1431, 'models': 1432, 'lambda': 1433, 'lognorm': 1434, 'zero': 1435, 'fx': 1436, 'reinitialization': 1437, 'neuron': 1438, 'latent': 1439, 'against': 1440, 'recruited': 1441, 'line': 1442, 'confuse': 1443, 'typical': 1444, 'scalability': 1445, 'billion': 1446, 'stuck': 1447, 'commonlyused': 1448, 'english': 1449, 'needed': 1450, 'minimal': 1451, 'untied': 1452, 'aggregation': 1453, 'certain': 1454, 'datapoints': 1455, 'including': 1456, 'massively': 1457, 'acoustic': 1458, 'dnns': 1459, 'reconsidering': 1460, 'operate': 1461, 'training': 1462, 'bound': 1463, 'key': 1464, 'gated': 1465, 'called': 1466, 'extremely': 1467, 'policies': 1468, 'effects': 1469, 'eval92': 1470, 'ratio': 1471, 'trivially': 1472, 'molecular': 1473, 'pseudoensemble': 1474, 'usps': 1475, 'optimize': 1476, 'equations': 1477, 'connection': 1478, 'reuses': 1479, 'issues': 1480, 'sequential': 1481, 'paradigms': 1482, 'specifications': 1483, 'frequency': 1484, 'intuitions': 1485, 'emphunfold': 1486, 'maintaing': 1487, 'turns': 1488, 'distributed': 1489, 'be': 1490, 'also': 1491, 'suitable': 1492, 'thoulessandersonpalmer': 1493, 'particle': 1494, 'objective': 1495, 'rules': 1496, 'improvement': 1497, 'forward': 1498, 'degradation': 1499, 'leverage': 1500, 'seek': 1501, 'extend': 1502, 'expensive': 1503, 'sped': 1504, 'implements': 1505, 'validate': 1506, 'repeated': 1507, 'fast': 1508, 'minimize': 1509, 'implicit': 1510, 'temporal': 1511, 'fixed': 1512, 'execution': 1513, 'variants': 1514, 'conventional': 1515, 'reported': 1516, 'noise': 1517, 'actors': 1518, 'resurgence': 1519, 'consistency': 1520, 'idea': 1521, 'modules': 1522, 'avoided': 1523, 'linear': 1524, 'teacher': 1525, 'relying': 1526, 'preventing': 1527, '347': 1528, 'highlevel': 1529, 'generated': 1530, 'minimum': 1531, 'attains': 1532, 'current': 1533, 'insights': 1534, 'segmented': 1535, 'tasks': 1536, 'surprising': 1537, 'samplespecific': 1538, 'prominence': 1539, 'construction': 1540, 'coherent': 1541, 'bit': 1542, 'channelout': 1543, 'offered': 1544, 'ambient': 1545, '393': 1546, 'realized': 1547, 'evidence': 1548, 'drops': 1549, 'improvements': 1550, 'them': 1551, 'deeper': 1552, 'empirically': 1553, 'finitesum': 1554, 'furthermore': 1555, 'foundation': 1556, 'losses': 1557, 'person': 1558, 'compute': 1559, 'put': 1560, 'subjects': 1561, 'fastfood': 1562, 'heavily': 1563, 'under': 1564, 'spike': 1565, 'normalized': 1566, 'timing': 1567, 'recovery': 1568, 'ensemble': 1569, 'extracted': 1570, 'gasfgadfmtf': 1571, 'relu': 1572, 'attention': 1573, 'picking': 1574, 'adjusting': 1575, 'robbins': 1576, 'usefulness': 1577, 'problemdomain': 1578, 'decision': 1579, 'interactions': 1580, 'findings': 1581, 'logmel': 1582, 'street': 1583, 'strides': 1584, 'maximum': 1585, 'labels': 1586, 'orders': 1587, 'cnn': 1588, 'rcv1': 1589, 'lowdimensional': 1590, 'nested': 1591, 'ph': 1592, 'topdown': 1593, 'optimized': 1594, 'rfn': 1595, 'provides': 1596, 'online': 1597, 'studying': 1598, 'nonparametric': 1599, 'variables': 1600, 'area': 1601, 'binary': 1602, 'speeds': 1603, 'computation': 1604, 'alternately': 1605, 'even': 1606, 'library': 1607, 'unified': 1608, 'datadriven': 1609, 'counter': 1610, 'three': 1611, 'model': 1612, 'visual': 1613, 'convex': 1614, 'primary': 1615, 'format': 1616, 'compresses': 1617, 'vision': 1618, 'thousands': 1619, 'last': 1620, 'interpretation': 1621, 'structureactivityproperty': 1622, 'mlp': 1623, 'focus': 1624, 'future': 1625, 'revealed': 1626, 'quantitative': 1627, 'activeinactive': 1628, 'corpora': 1629, 'foregrounds': 1630, 'route': 1631, 'out': 1632, 'look': 1633, 'analysis': 1634, 'hours': 1635, 'descent': 1636, 'sparselyconnected': 1637, 'realworld': 1638, 'sample': 1639, '50hr': 1640, 'words': 1641, 'inside': 1642, 'additional': 1643, 'actions': 1644, 'distilling': 1645, 'substantially': 1646, 'reinitialised': 1647, 'degree': 1648, 'sampled': 1649, 'with': 1650, 'choice': 1651, 'paving': 1652, 'autoregressive': 1653, 'benefits': 1654, 'recognizer': 1655, 'sequences': 1656, 'logging': 1657, 'markov': 1658, 'hierarchy': 1659, 'error': 1660, 'problems': 1661, 'krylov': 1662, 'particular': 1663, 'spent': 1664, '402': 1665, 'crafted': 1666, 'meaning': 1667, 'chainrule': 1668, 'misclassified': 1669, 'processor': 1670, 'nine': 1671, 'compounds': 1672, 'most': 1673, 'seriously': 1674, 'signals': 1675, 'call': 1676, 'et': 1677, 'stochastic': 1678, 'find': 1679, 'adaptively': 1680, 'facilitates': 1681, 'when': 1682, 'utilities': 1683, 'allows': 1684, 'related': 1685, 'cascade': 1686, 'minimization': 1687, 'fingerprints': 1688, 'shadow': 1689, 'nesterov': 1690, 'decade': 1691, 'qsarqspr': 1692, 'modified': 1693, 'standard': 1694, 'misclassification': 1695, 'magnitude': 1696, 'renormalization': 1697, 'parallelize': 1698, 'cnns': 1699, 'within': 1700, 'python': 1701, 'communicated': 1702, 'preserved': 1703, 'proposed': 1704, 'investigate': 1705, 'masks': 1706, 'what': 1707, 'ligandbased': 1708, 'maintaining': 1709, 'augmenting': 1710, 'vulnerable': 1711, 'parallelization': 1712, 'constraints': 1713, 'convolutional': 1714, 'theanos': 1715, 'influencing': 1716, 'orderings': 1717, 'gates': 1718, 'optima': 1719, 'runs': 1720, 'they': 1721, 'that': 1722, 'space': 1723, 'such': 1724, 'submodels': 1725, 'regularizing': 1726, 'wavelet': 1727, 'manipulated': 1728, 'hiddentohidden': 1729, 'mitigated': 1730, 'analyze': 1731, 'biological': 1732, 'conjectured': 1733, 'does': 1734, '578': 1735, 'augmentation': 1736, 'journal': 1737, 'modern': 1738, 'accuracy': 1739, 'constraint': 1740, 'play': 1741, 'partly': 1742, '3': 1743, 'transformation': 1744, 'first': 1745, 'intent': 1746, 'differently': 1747, 'shows': 1748, 'autoencoder': 1749, 'experimental': 1750, 'accelerator': 1751, 'techniques': 1752, 'dependence': 1753, 'feed': 1754, 'precision': 1755, 'enjoy': 1756, 'kullbackleibler': 1757, 'may': 1758, 'energy': 1759, 'number': 1760, 'supervised': 1761, 'introduced': 1762, 'determining': 1763, 'his': 1764, 'stateoftheart': 1765, 'costly': 1766, 'outmatched': 1767, 'distancepreserving': 1768, 'minimizing': 1769, 'layers': 1770, 'competitive': 1771, 'topics': 1772, 'intermediate': 1773, 'margin': 1774, 'krizhevsky': 1775, 'given': 1776, 'from': 1777, 'modelling': 1778, 'deepshallow': 1779, 'performed': 1780, 'allow': 1781, 'clusters': 1782, 'pruning': 1783, 'firstorder': 1784, 'filters': 1785, 'graphs': 1786, 'mapreduce': 1787, 'loss': 1788, 'solidly': 1789, 'itself': 1790, '20': 1791, 'predicts': 1792, 'if': 1793, 'extension': 1794, 'versions': 1795, 'reconstructed': 1796, 'summarizes': 1797, 'attributed': 1798, 'top': 1799, 'dude': 1800, 'assign': 1801, 'length': 1802, 'translation': 1803, 'relation': 1804, 'robustness': 1805, 'inner': 1806, 'gene': 1807, 'close': 1808, 'benchmarks': 1809, 'embarrassingly': 1810, 'single': 1811, 'mdrnns': 1812, 'rotation': 1813, 'knowledge': 1814, 'relative': 1815, 'regular': 1816, 'gprop': 1817, 'recent': 1818, 'superelliptic': 1819, 'continuous': 1820, 'themes': 1821, 'combining': 1822, 'whereby': 1823, 'scheme': 1824, 'unsurprisingly': 1825, 'potentially': 1826, 'interpreted': 1827, 'derivatives': 1828, 'implemented': 1829, 'regarding': 1830, 'used': 1831, 'exploit': 1832, 'cannot': 1833, 'specific': 1834, 'serial': 1835, 'regularization': 1836, 'hybrid': 1837, 'quasinewton': 1838, 'solver': 1839, 'ability': 1840, 'interesting': 1841, 'important': 1842, 'overall': 1843, 'subject': 1844, 'fact': 1845, 'modest': 1846, 'questions': 1847, 'fly': 1848, 'balance': 1849, 'undermine': 1850, 'svm': 1851, 'tolerate': 1852, 'locality': 1853, 'belief': 1854, 'special': 1855, 'adaptation': 1856, 'mpi': 1857, 'illustrate': 1858, 'iteration': 1859, 'attack': 1860, 'whole': 1861, 'projections': 1862, 'demonstrates': 1863, 'application': 1864, 'optimisers': 1865, 'arises': 1866, 'iii': 1867, 'initialization': 1868, 'argue': 1869, 'employ': 1870, 'by': 1871, 'ctc': 1872, 'activate': 1873, 'dependent': 1874, 'essential': 1875, 'process': 1876, '12': 1877, 'conjugate': 1878, 'rounding': 1879, 'cifbased': 1880, 'memory': 1881, 'normalizing': 1882, 'private': 1883, 'accepted': 1884, 'global': 1885, 'ie': 1886, 'inspired': 1887, 'routinely': 1888, 'pruned': 1889, 'latter': 1890, 'tolerates': 1891, 'involving': 1892, 'specifically': 1893, 'introduce': 1894, 'load': 1895, 'architectures': 1896, 'challenge': 1897, 'collection': 1898, 'attaching': 1899, 'exact': 1900, 'gfrnn': 1901, 'singlelayer': 1902, 'place': 1903, 'focused': 1904, 'bijection': 1905, 'understanding': 1906, 'encoder': 1907, 'wider': 1908, 'offers': 1909, 'outperformed': 1910, 'evident': 1911, 'guided': 1912, 'note': 1913, 'arithmetic': 1914, 'us': 1915, 'nonconvexity': 1916, 'permutationinvariant': 1917, 'amenable': 1918, 'exchanged': 1919, 'unknown': 1920, 'open': 1921, 'tricks': 1922, 'compression': 1923, 'parallel': 1924, 'asr': 1925, 'flows': 1926, 'datasetsthese': 1927, 'parametric': 1928, 'greatest': 1929, 'pattern': 1930, 'msecross': 1931, 'actively': 1932, 'faithfully': 1933, 'relaxing': 1934, 'entry': 1935, 'inspiration': 1936, 'domains': 1937, 'metropolishastings': 1938, 'establishing': 1939, 'output': 1940, 'density': 1941, 'schemes': 1942, 'baseline': 1943, 'discriminative': 1944, 'methods': 1945, 'enhancement': 1946, 'aspects': 1947, 'require': 1948, 'qsar': 1949, 'reversal': 1950, 'has': 1951, 'soft': 1952, 'curvaturevector': 1953, '15': 1954, 'found': 1955, 'gained': 1956, 'onehidden': 1957, 'embeddings': 1958, 'distillation': 1959, 'characterization': 1960, 'hessianfree': 1961, 'stack': 1962, 'criterion': 1963, 'behavior': 1964, 'partially': 1965, 'parallelism': 1966, 'reach': 1967, 'suggest': 1968, 'takes': 1969, 'reasonable': 1970, 'implementing': 1971, 'learnability': 1972, 'theoretically': 1973, 'regularizer': 1974, 'outperforms': 1975, 'bootstrap': 1976, 'addition': 1977, 'fit': 1978, 'threespeaker': 1979, 'individual': 1980, 'showed': 1981, 'rapidly': 1982, 'altering': 1983, 'trained': 1984, 'combines': 1985, 'physical': 1986, 'encounter': 1987, 'gasfgadf': 1988, 'inversely': 1989, 'widelypopular': 1990, 'avoiding': 1991, 'mbn': 1992, 'granted': 1993, 'machines': 1994, 'beneficial': 1995, 'smallscale': 1996, '45': 1997, 'harder': 1998, 'objectives': 1999, 'm': 2000, 'movement': 2001, 'train': 2002, 'runtime': 2003, 'rendering': 2004, 'core': 2005, 'careful': 2006, 'spawned': 2007, 'understand': 2008, 'tend': 2009, 'iterative': 2010, 'svhn': 2011, 'humans': 2012, 'artificial': 2013, 'modelbased': 2014, 'factor': 2015, 'mechanism': 2016, 'promotes': 2017, 'detect': 2018, 'drawing': 2019, 'nondifferentiable': 2020, 'semisupervised': 2021, 'raw': 2022, 'fundamental': 2023, 'ranked': 2024, 'hurt': 2025, 'distinguish': 2026, 'ideal': 2027, 'beyond': 2028, 'deep': 2029, 'convergence': 2030, 'dynamic': 2031, 'themselves': 2032, 'literature': 2033, 'fcdot': 2034, 'byproduct': 2035, '3083': 2036, 'orthogonal': 2037, 'progresses': 2038, 'backward': 2039, 'shallow': 2040, 'development': 2041, 'autonomous': 2042, 'results': 2043, 'ranging': 2044, '97': 2045, 'reduced': 2046, 'fmllr': 2047, 'offer': 2048, 'rfns': 2049, 'employed': 2050, 'cyclic': 2051, 'stale': 2052, 'surged': 2053, 'pitch': 2054, 'fame': 2055, 'consisting': 2056, 'factorizes': 2057, 'captures': 2058, 'denoised': 2059, 'hiddentooutput': 2060, 'researchers': 2061, 'logarithmicregret': 2062, 'behavioral': 2063, 'new': 2064, 'adapt': 2065, 'procedure': 2066, 'crashed': 2067, 'realtime': 2068, 'abound': 2069, '1030': 2070, 'risk': 2071, 'deployment': 2072, 'functions': 2073, 'whose': 2074, 'martens': 2075, 'computational': 2076, 'minimizations': 2077, 'sustainability': 2078, 'geometrical': 2079, 'object': 2080, 'periodically': 2081, 'those': 2082, 'performing': 2083, 'schmidhuber': 2084, 'eg': 2085, 'finally': 2086, 'role': 2087, 'team': 2088, 'fear': 2089, 'complexvalued': 2090, 'contains': 2091, 'phonemes': 2092, 'term': 2093, 'heldout': 2094, 'backpropagationby': 2095, 'usually': 2096, 'how': 2097, 'churn': 2098, 'support': 2099, 'serialization': 2100, 'omegan13': 2101, 'example': 2102, 'designed': 2103, 'gradientbased': 2104, 'evaluate': 2105, 'generate': 2106, 'inputspace': 2107, 'pretrain': 2108, 'extensive': 2109, 'implies': 2110, 'able': 2111, 'perexample': 2112, 'separate': 2113, 'standardize': 2114, 'gamma': 2115, 'motivating': 2116, 'fxneq': 2117, 'fuel': 2118, 'driven': 2119, 'penaltybased': 2120, 'not': 2121, 'platforms': 2122, 'randomly': 2123, 'ideally': 2124, 'gate': 2125, 'in': 2126, 'compiler': 2127, 'advantage': 2128, 'was': 2129, 'versatility': 2130, 'built': 2131, 'embed': 2132, 'possible': 2133, 'auc': 2134, 'anrat': 2135, 'neural': 2136, 'nonasymptotic': 2137, 'subsequently': 2138, 'formulation': 2139, 'difficulty': 2140, 'wise': 2141, 'rectifier': 2142, 'processing': 2143, 'research': 2144, 'craft': 2145, 'approximations': 2146, 'massive': 2147, 'resnets': 2148, 'measures': 2149, 'treat': 2150, 'voice': 2151, 'dbns': 2152, 'expression': 2153, 'directly': 2154, 'activations': 2155, 'choices': 2156, 'efficient': 2157, 'al': 2158, 'dnn': 2159, 'phone': 2160, 'demonstrated': 2161, 'guarantees': 2162, 'rnn': 2163, 'numbers': 2164, 'millions': 2165, 'parts': 2166, 'features': 2167, 'information': 2168, 'sparsity': 2169, 'inherently': 2170, 'signal': 2171, 'complexity': 2172, 'few': 2173, 'ultimate': 2174, 'ondemand': 2175, '2100': 2176, 'observed': 2177, 'ten': 2178, 'static': 2179, 'equal': 2180, 'alternative': 2181, 'bidirectional': 2182, 'ample': 2183, 'o1varepsilon2': 2184, 'cortical': 2185, 'items': 2186, 'domain': 2187, 'concept': 2188, 'satisfied': 2189, 'recurrent': 2190, 'algebra': 2191, 'assays': 2192, 'equipped': 2193, 'widely': 2194, 'expert': 2195, 'initialized': 2196, 'synthesized': 2197, 'nodes': 2198, 'negligible': 2199, 'backpropagated': 2200, 'alignment': 2201, 'presented': 2202, 'frames': 2203, 'secondly': 2204, 'overlapping': 2205, 'captured': 2206, 'wsj': 2207, 'defining': 2208, 'decays': 2209, 'essay': 2210, 'hindered': 2211, 'sentiment': 2212, 'adagrad': 2213, 'sign': 2214, 'convenient': 2215, 'calculations': 2216, 'pre': 2217, 'established': 2218, 'capture': 2219, 'majority': 2220, 'redundancy': 2221, 'one': 2222, 'seeks': 2223, 'were': 2224, 'much': 2225, 'alternate': 2226, 'alternates': 2227, 'business': 2228, 'success': 2229, 'converges': 2230, 'providing': 2231, 'activation': 2232, 'representations': 2233, 'matrix': 2234, 'cmoscompatible': 2235, 'speech': 2236, 'adversaryselected': 2237, 'dimensions': 2238, 'almost': 2239, 'mathbbrm': 2240, 'unlabeled': 2241, 'p': 2242, 'discovery': 2243, 'hardware': 2244, 'shortcomings': 2245, 'coadaptation': 2246, 'sounds': 2247, 'especially': 2248, 'vocabulary': 2249, 'task': 2250, 'quantify': 2251, 'previously': 2252, 'f': 2253, 'suited': 2254, 'predicting': 2255, 'highenergy': 2256, 'action': 2257, 'measure': 2258, 'widespread': 2259, 'placed': 2260, 'probout': 2261, 'weight': 2262, 'batches': 2263, 'years': 2264, 'clarifies': 2265, 'subclass': 2266, 'considered': 2267, 'hypothesized': 2268, 'processors': 2269, 'finding': 2270, 'challenging': 2271, 'processes': 2272, 'tractable': 2273, 'augmented': 2274, 'reveal': 2275, 'it': 2276, 'ties': 2277, 'search': 2278, 'though': 2279, 'stage': 2280, 'h': 2281, 'via': 2282, 'motivated': 2283, 'employment': 2284, 'investigates': 2285, 'percentage': 2286, 'low': 2287, 'contributes': 2288, 'interact': 2289, 'update': 2290, 'result': 2291, 'detectors': 2292, 'achievable': 2293, 'decisions': 2294, 'today': 2295, 'remarkably': 2296, 'kadanoff': 2297, 'rather': 2298, 'relevant': 2299, 'criteria': 2300, 'perform': 2301, 'community': 2302, 'spatial': 2303, 'eigendecompositions': 2304, 'universally': 2305, 'collaborators': 2306, 'iterate': 2307, 'manifold': 2308, 'art': 2309, 'essentially': 2310, 'dominated': 2311, 'amounts': 2312, 'general': 2313, 'scales': 2314, 'lie': 2315, 'valpola': 2316, 'filtering': 2317, 'zeroth': 2318, 'successes': 2319, 'integrate': 2320, 'nontrivial': 2321, 'symmetries': 2322, 'provably': 2323, 'twolayer': 2324, 'da': 2325, 'theoretical': 2326, 'graphics': 2327, 'rudra': 2328, 'over': 2329, 'whether': 2330, 'bioactivity': 2331, 'lower': 2332, 'perhaps': 2333, 'interest': 2334, 'estimates': 2335, 'factorizations': 2336, 'integration': 2337, 'euclidean': 2338, 'hmmbased': 2339, 'onchip': 2340, 'introduces': 2341, 'partition': 2342, 'sharing': 2343, 'incrementally': 2344, 'parallelized': 2345, 'second': 2346, 'distribution': 2347, 'stories': 2348, 'code': 2349, 'method': 2350, 'implementation': 2351, 'ica': 2352, 'for': 2353, 'innovative': 2354, 'images': 2355, 'breed': 2356, 'easily': 2357, 'pair': 2358, 'makes': 2359, 'noisy': 2360, 'symbols': 2361, 'proper': 2362, 'estimations': 2363, 'hidden': 2364, 'encoding': 2365, 'ddimensional': 2366, 'far': 2367, 'connectivity': 2368, 'determined': 2369, 'classical': 2370, 'seminmfs': 2371, 'bp': 2372, 'bypass': 2373, 'nearby': 2374, 'adaptive': 2375, 'nor': 2376, 'any': 2377, 'walk': 2378, 'ladder': 2379, 'caffe': 2380, 'fullysupervised': 2381, 'coarse': 2382, 'mutually': 2383, 'posterior': 2384, 'difficulties': 2385, 'rank': 2386, 'react': 2387, 'infeasible': 2388, 'tuned': 2389, 'classification': 2390, 'precise': 2391, 'circular': 2392, 'decouple': 2393, 'applies': 2394, 'solution': 2395, 'naturally': 2396, 'independently': 2397, 'theano': 2398, 'using': 2399, 'increases': 2400, 'ffn': 2401, 'mass': 2402, 'streaming': 2403, 'orientation': 2404, 'outputting': 2405, 'forests': 2406, 'test': 2407, 'ii': 2408, 'critical': 2409, 'engineering': 2410, 'principled': 2411, 'settings': 2412, 'mnist': 2413, 'multiple': 2414, 'highdimensional': 2415, 'computed': 2416, 'isometry': 2417, 'bm': 2418, 'das': 2419, 'decompose': 2420, 'intimately': 2421, 'activating': 2422, 'sequence': 2423, 'device': 2424, 'enhances': 2425, 'approximated': 2426, 'targets': 2427, 'induces': 2428, 'intuitively': 2429, 'unrealistic': 2430, 'stacking': 2431, 'timescales': 2432, 'would': 2433, 'extent': 2434, 'series': 2435, 'stabilizer': 2436, 'etc': 2437, 'form': 2438, 'deploy': 2439, 'propagation': 2440, 'sensors': 2441, 'pseudo': 2442, 'lightweight': 2443, 'nonoverlapping': 2444, 'correctly': 2445, 'polyphonic': 2446, 'care': 2447, 'besides': 2448, 'problem': 2449, 'surprisingly': 2450, 'greater': 2451, 'neurons': 2452, 'hyperparameter': 2453, 'leveraging': 2454, 'composed': 2455, 'guaranteed': 2456, 'builds': 2457, 'estimated': 2458, 'projects': 2459, 'random': 2460, 'shortcut': 2461, 'produces': 2462, 'distributing': 2463, 'perturbing': 2464, 'belongs': 2465, 'hundreds': 2466, 'consisted': 2467, 'than': 2468, 'minimized': 2469, 'circuits': 2470, 'mdrnn': 2471, 'solvers': 2472, 'inputs': 2473, 'component': 2474, 'synthesizing': 2475, 'fried': 2476, 'intermediary': 2477, 'match': 2478, 'based': 2479, '400hour': 2480, 'direct': 2481, 'matching': 2482, 'learningtrainingoptimization': 2483, 'resource': 2484, 'stabilizers': 2485, 'helpful': 2486, 'forcing': 2487, 'achieved': 2488, 'strategies': 2489, 'solve': 2490, 'analogous': 2491, 'locally': 2492, 'analytically': 2493, 'erms': 2494, 'parametrized': 2495, 'experimentally': 2496, 'fourth': 2497, 'faster': 2498, 'comprise': 2499, 'internet': 2500, '6db': 2501, '300': 2502, 'superhuman': 2503, 'drug': 2504, 'accurate': 2505, 'docking': 2506, 'imputation': 2507, 'an': 2508, 'might': 2509, 'adversaries': 2510, 'leading': 2511, 'mechanisms': 2512, 'predictor': 2513, 'contain': 2514, 'is': 2515, 'power': 2516, 'whereas': 2517, 'channel': 2518, 'scientific': 2519, 'learning': 2520, 'entropy': 2521, 'operators': 2522, 'prone': 2523, 'applied': 2524, 'dimension': 2525, 'type': 2526, 'nonlinear': 2527, 'max': 2528, 'o1varepsilon': 2529, '95': 2530, 'named': 2531, 'alleviates': 2532, 'field': 2533, 'along': 2534, 'rimes': 2535, 'several': 2536, 'making': 2537, 'traditionally': 2538, 'efforts': 2539, 'classifiers': 2540, 'conduct': 2541, 'pxh': 2542, 'gd': 2543, 'cases': 2544, 'become': 2545, 'all': 2546, 'defense': 2547, 'evaluations': 2548, 'resistive': 2549, 'however': 2550, 'simpler': 2551, 'validated': 2552, 'diverse': 2553, 'outofsample': 2554, 'confidence': 2555, 'but': 2556, 'monitoring': 2557, 'ip': 2558, 'many': 2559, 'assist': 2560, 'structure': 2561, 'specified': 2562, 'divergence': 2563, 'sensory': 2564, 'submodel': 2565, 'gigaopssw': 2566, 'equaled': 2567, 'comparable': 2568, 'selecting': 2569, 'outperform': 2570, 'dropout': 2571, 'enormous': 2572, 'applying': 2573, 'start': 2574, 'larger': 2575, 'affect': 2576, 'decomposition': 2577, 'strong': 2578, 'unclear': 2579, 'selected': 2580, 'unlike': 2581, 'mature': 2582, 'less': 2583, 'thereby': 2584, 'types': 2585, 'arguing': 2586, 'level': 2587, 'environment': 2588, 'prevent': 2589, 'similar': 2590, 'experts': 2591, 'setting': 2592, 'practices': 2593, 'interplay': 2594, 'alignments': 2595, 'vast': 2596, 'at': 2597, 'outputs': 2598, 'regression': 2599, 'context': 2600, 'conditional': 2601, 'gaps': 2602, 'framework': 2603, 'although': 2604, 'datasets': 2605, 'central': 2606, 'evolved': 2607, 'restarting': 2608, 'ising': 2609, 'adadelta': 2610, 'hfx': 2611, 'early': 2612, 'networks': 2613, 'sparse': 2614, 'box': 2615, 'datacentersize': 2616, 'exceeding': 2617, 'rest': 2618, 'samm': 2619, 'binding': 2620, 'case': 2621, 'effectiveness': 2622, 'boundary': 2623, 'observation': 2624, 'versatile': 2625, 'definitions': 2626, 'fixedpoint': 2627, '2015': 2628, 'best': 2629, 'utilize': 2630, 'tackle': 2631, 'adversarial': 2632, 'sbm': 2633, 'upto': 2634, 'dense': 2635, 'implementations': 2636, 'differentially': 2637, 'transforms': 2638, 'uncover': 2639, 'distributions': 2640, 'main': 2641, 'multinomial': 2642, 'higgs': 2643, 'classifier': 2644, 'speakers': 2645, 'broad': 2646, '2013': 2647, 'ordering': 2648, 'produced': 2649, 'fits': 2650, 'this': 2651, 'recordbreaking': 2652, 'identify': 2653, 'devices': 2654, 'compound': 2655, 'beginning': 2656, 'concentrates': 2657, 'without': 2658, 'misclassify': 2659, 'joint': 2660, 'tiled': 2661, 'factors': 2662, 'embeds': 2663, 'paper': 2664, 'bounds': 2665, 'exhibits': 2666, 'iot': 2667, 'reasons': 2668, 'specialist': 2669, 'dbn': 2670, 'inclass': 2671, 'offline': 2672, 'nesting': 2673, 'predictive': 2674, 'multidimensional': 2675, 'ways': 2676, 'optimal': 2677, 'simplest': 2678, 'characterlevel': 2679, 'users': 2680, 'meanfield': 2681, 'integrates': 2682, 'assumed': 2683, 'coupled': 2684, 'accurately': 2685, 'mnst': 2686, 'smaller': 2687, '16bit': 2688, 'switchboard': 2689, 'transparency': 2690, 'evolving': 2691, 'rpu': 2692, 'generally': 2693}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Aw7bxIHLrW"
      },
      "source": [
        "# Splitting data into Train sets and test sets\r\n",
        "\r\n",
        "X = [] \r\n",
        "y = []\r\n",
        "\r\n",
        "for c in lines:\r\n",
        "  xxxx = c.replace('\\n','').split(' ')\r\n",
        "  X.append(' '.join(xxxx[:-1])) # X from the corpus\r\n",
        "\r\n",
        "  yyyy = [0 for i in range(len(vocabulary))] # Generate Y from the Vocabulary\r\n",
        "  # yyyy[primary_store[xxxx[-1]]] = 1\r\n",
        "  yyyy[primary_store[xxxx[-1]]] = 1\r\n",
        "  y.append(yyyy)\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\r\n",
        "y_test = numpy.array(y_test)\r\n",
        "y_train = numpy.array(y_train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzeMJVLK-L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628226a1-19f5-4288-91a6-e7ada8294357"
      },
      "source": [
        "# PREVIEW OUTPUT ::\r\n",
        "\r\n",
        "print(X_train[:10])\r\n",
        "print(y_train[:10])\r\n",
        "print(X_test[:10])\r\n",
        "print(y_test[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['in this paper we present an infinite hierarchical nonparametric bayesian model to extract the hidden factors over observed data where the number of hidden factors for each layer is unknown and can be potentially infinite moreover the number of layers can also be infinite we construct the model structure that allows continuous values for the hidden factors and weights which makes the model suitable for various applications we use the metropolishastings method to infer the model structure then the performance of the algorithm is evaluated by the experiments simulation results show that the model fits the underlying structure of simulated', 'we study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have deep networks are able to sequentially map portions of each layers inputspace to the same output in this way deep models compute functions that react equally to complicated patterns of different inputs the compositional structure of these functions enables them to reuse pieces of computation exponentially often in terms of the networks depth this paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions in particular our analysis is not specific to a single family of models and as an example we employ it for rectifier and maxout networks we improve complexity bounds from preexisting work and investigate the behavior of units in higher', 'deep convolutional neural networks comprise a subclass of deep neural networks dnn with a constrained architecture that leverages the spatial and temporal structure of the domain they model convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models although dnns have been used in drug discovery for qsar and ligandbased bioactivity predictions none of these models have benefited from this powerful convolutional architecture this paper introduces atomnet the first structurebased deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications we demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions in further contrast to existing dnn techniques we show that atomnets application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators finally we show that atomnet outperforms previous docking approaches on a diverse set of benchmarks by a large margin achieving an auc greater than 09 on 578 of the targets in the dude', 'deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data recently such techniques have yielded recordbreaking results on a diverse set of difficult machine learning tasks in computer vision speech recognition and natural language processing despite the enormous success of deep learning relatively little is understood theoretically about why these techniques are so successful at feature learning and compression here we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics the renormalization group rg rg is an iterative coarsegraining scheme that allows for the extraction of relevant features ie operators as a physical system is examined at different length scales we construct an exact mapping from the variational renormalization group first introduced by kadanoff and deep learning architectures based on restricted boltzmann machines rbms we illustrate these ideas using the nearestneighbor ising model in one and twodimensions our results suggests that deep learning algorithms may be employing a generalized rglike scheme to learn relevant features from', 'deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data recently such techniques have yielded recordbreaking results on a diverse set of difficult machine learning tasks in computer vision speech recognition and natural language processing despite the enormous success of deep learning relatively little is understood theoretically about why these techniques are so successful at feature learning and compression here we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics the renormalization group rg rg is an iterative coarsegraining scheme that allows for the extraction of relevant features ie operators as a physical system is examined at different length scales we construct an exact mapping from the variational renormalization group first introduced by kadanoff and deep learning architectures based on restricted boltzmann machines rbms we illustrate these ideas using the nearestneighbor ising model in one and twodimensions our results suggests that deep learning algorithms may be employing a generalized rglike scheme to learn relevant features from', 'the resilient propagation rprop algorithm has been very popular for backpropagation training of multilayer feedforward neural networks in various applications the standard rprop however encounters difficulties in the context of deep neural networks as typically happens with gradientbased learning algorithms in this paper we propose a modification of the rprop that combines standard rprop steps with a special drop out technique we apply the method for training deep neural networks as standalone components and in ensemble formulations results on the mnist dataset show that the proposed modification alleviates standard rprops problems demonstrating improved learning speed and', 'deep belief networks dbn have been successfully applied on popular machine learning tasks specifically when applied on handwritten digit recognition dbns have achieved approximate accuracy rates of 988 in an effort to optimize the data representation achieved by the dbn and maximize their descriptive power recent advances have focused on inducing sparse constraints at each layer of the dbn in this paper we present a theoretical approach for sparse constraints in the dbn using the mixed norm for both nonoverlapping and overlapping groups we explore how these constraints affect the classification accuracy for digit recognition in three different datasets mnist usps rimes and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap', 'heuristic optimisers which search for an optimal configuration of variables relative to an objective function often get stuck in local optima where the algorithm is unable to find further improvement the standard approach to circumvent this problem involves periodically restarting the algorithm from random initial configurations when no further improvement can be found we propose a method of partial reinitialization whereby in an attempt to find a better solution only subsets of variables are reinitialised rather than the whole configuration much of the information gained from previous runs is hence retained this leads to significant improvements in the quality of the solution found in a given time for a variety of optimisation problems in machine', 'why does deep learning work what representations does it capture how do higherorder representations emerge we study these questions from the perspective of group theory thereby opening a new approach towards a theory of deep learning   one factor behind the recent resurgence of the subject is a key algorithmic step called em pretraining first search for a good generative model for the input samples and repeat the process one layer at a time we show deeper implications of this simple principle by establishing a connection with the interplay of orbits and stabilizers of group actions although the neural networks themselves may not form groups we show the existence of em shadow groups whose elements serve as close approximations   over the shadow groups the pretraining step originally introduced as a mechanism to better initialize a network becomes equivalent to a search for features with minimal orbits intuitively these features are in a way the em simplest which explains why a deep learning network learns simple features first next we show how the same principle when repeated in the deeper layers can capture higher order representations and why representation complexity increases as the layers get', 'in a physical neural system where storage and processing are intimately intertwined the rules for adjusting the synaptic weights can only depend on variables that are available locally such as the activity of the pre and postsynaptic neurons resulting in local learning rules a systematic framework for studying the space of local learning rules must first define the nature of the local variables and then the functional form that ties them together into each learning rule we consider polynomial local learning rules and analyze their behavior and capabilities in both linear and nonlinear networks as a byproduct this framework enables also the discovery of new learning rules as well as important relationships between learning rules and group symmetries stacking local learning rules in deep feedforward networks leads to deep local learning while deep local learning can learn interesting representations it cannot learn complex inputoutput functions even when targets are available for the top layer learning complex inputoutput functions requires local deep learning where target information is propagated to the deep layers through a backward channel the nature of the propagated information about the targets and the backward channel through which this information is propagated partition the space of learning algorithms for any learning algorithm the capacity of the backward channel can be defined as the number of bits provided about the gradient per weight divided by the number of required operations per weight we estimate the capacity associated with several learning algorithms and show that backpropagation outperforms them and achieves the maximum possible capacity the theory clarifies the concept of hebbian learning what is learnable by hebbian learning and explains the sparsity of the space of learning rules discovered so']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data recently such techniques have yielded recordbreaking results on a diverse set of difficult machine learning tasks in computer vision speech recognition and natural language processing despite the enormous success of deep learning relatively little is understood theoretically about why these techniques are so successful at feature learning and compression here we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics the renormalization group rg rg is an iterative coarsegraining scheme that allows for the extraction of relevant features ie operators as a physical system is examined at different length scales we construct an exact mapping from the variational renormalization group first introduced by kadanoff and deep learning architectures based on restricted boltzmann machines rbms we illustrate these ideas using the nearestneighbor ising model in one and twodimensions our results suggests that deep learning algorithms may be employing a generalized rglike scheme to learn relevant features from', 'a network supporting deep unsupervised learning is presented the network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy the lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features while standard autoencoders are analogous to latent variable models with a single layer of stochastic variables the proposed network is analogous to hierarchical latent variables models learning combines denoising autoencoder and denoising sources separation frameworks each layer of the network contributes to the cost function a term which measures the distance of the representations produced by the encoder and the decoder since training signals originate from all levels of the network all layers can learn efficiently even in deep networks the speedup offered by cost terms from higher levels of the hierarchy and the ability to learn invariant features are demonstrated in', 'deep neural network dnn acoustic models have yielded many stateoftheart results in automatic speech recognition asr tasks more recently recurrent neural network rnn models have been shown to outperform dnns counterparts however stateoftheart dnn and rnn models tend to be impractical to deploy on embedded systems with limited computational capacity traditionally the approach for embedded platforms is to either train a small dnn directly or to train a small dnn that learns the output distribution of a large dnn in this paper we utilize a stateoftheart rnn to transfer knowledge to small dnn we use the rnn model to generate soft alignments and minimize the kullbackleibler divergence against the small dnn the small dnn trained on the soft rnn alignments achieved a 393 wer on the wall street journal wsj eval92 task compared to a baseline 454 wer or more than 13 relative', 'in this paper we present an infinite hierarchical nonparametric bayesian model to extract the hidden factors over observed data where the number of hidden factors for each layer is unknown and can be potentially infinite moreover the number of layers can also be infinite we construct the model structure that allows continuous values for the hidden factors and weights which makes the model suitable for various applications we use the metropolishastings method to infer the model structure then the performance of the algorithm is evaluated by the experiments simulation results show that the model fits the underlying structure of simulated', 'three important properties of a classification machinery are i the system preserves the core information of the input data ii the training examples convey information about unseen data and iii the system is able to treat differently points from different classes in this work we show that these fundamental properties are satisfied by the architecture of deep neural networks we formally prove that these networks with random gaussian weights perform a distancepreserving embedding of the data with a special treatment for inclass and outofclass data similar points at the input of the network are likely to have a similar output the theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature thereby making a formal connection between these important topics the derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data the results are validated with stateoftheart trained', 'we introduce two python frameworks to train neural networks on large datasets blocks and fuel blocks is based on theano a linear algebra compiler with cudasupport it facilitates the training of complex neural network models by providing parametrized theano operations attaching metadata to theanos symbolic computational graph and providing an extensive set of utilities to assist training the networks eg training algorithms logging monitoring visualization and serialization fuel provides a standard format for machine learning datasets it allows the user to easily iterate over large datasets performing many types of preprocessing on the', 'modelbased methods and deep neural networks have both been tremendously successful paradigms in machine learning in modelbased methods problem domain knowledge can be built into the constraints of the model typically at the expense of difficulties during inference in contrast deterministic deep neural networks are constructed in such a way that inference is straightforward but their architectures are generic and it is unclear how to incorporate knowledge this work aims to obtain the advantages of both approaches to do so we start with a modelbased approach and an associated inference algorithm and emphunfold the inference iterations as layers in a deep network rather than optimizing the original model we emphuntie the model parameters across layers in order to create a more powerful network the resulting architecture can be trained discriminatively to perform accurate inference within a fixed network size we show how this framework allows us to interpret conventional networks as meanfield inference in markov random fields and to obtain new architectures by instead using belief propagation as the inference algorithm we then show its application to a nonnegative matrix factorization model that incorporates the problemdomain knowledge that sound sources are additive deep unfolding of this model yields a new kind of nonnegative deep neural network that can be trained using a multiplicative backpropagationstyle update algorithm we present speech enhancement experiments showing that our approach is competitive with conventional neural networks despite using far fewer', 'we have recently shown that deep long shortterm memory lstm recurrent neural networks rnns outperform feed forward deep neural networks dnns as acoustic models for speech recognition more recently we have shown that the performance of sequence trained context dependent cd hidden markov model hmm acoustic models using such lstm rnns can be equaled by sequence trained phone models initialized with connectionist temporal classification ctc in this paper we present techniques that further improve performance of lstm rnn acoustic models for large vocabulary speech recognition we show that frame stacking and reduced frame rate lead to more accurate models and faster decoding cd phone modeling leads to further improvements we also present initial results for lstm rnn models outputting words', 'in a physical neural system where storage and processing are intimately intertwined the rules for adjusting the synaptic weights can only depend on variables that are available locally such as the activity of the pre and postsynaptic neurons resulting in local learning rules a systematic framework for studying the space of local learning rules must first define the nature of the local variables and then the functional form that ties them together into each learning rule we consider polynomial local learning rules and analyze their behavior and capabilities in both linear and nonlinear networks as a byproduct this framework enables also the discovery of new learning rules as well as important relationships between learning rules and group symmetries stacking local learning rules in deep feedforward networks leads to deep local learning while deep local learning can learn interesting representations it cannot learn complex inputoutput functions even when targets are available for the top layer learning complex inputoutput functions requires local deep learning where target information is propagated to the deep layers through a backward channel the nature of the propagated information about the targets and the backward channel through which this information is propagated partition the space of learning algorithms for any learning algorithm the capacity of the backward channel can be defined as the number of bits provided about the gradient per weight divided by the number of required operations per weight we estimate the capacity associated with several learning algorithms and show that backpropagation outperforms them and achieves the maximum possible capacity the theory clarifies the concept of hebbian learning what is learnable by hebbian learning and explains the sparsity of the space of learning rules discovered so', 'we introduce a new representation learning algorithm suited to the context of domain adaptation in which data at training and test time come from similar but different distributions our algorithm is directly inspired by theory on domain adaptation suggesting that for effective domain transfer to be achieved predictions must be made based on a data representation that cannot discriminate between the training source and test target domains we propose a training objective that implements this idea in the context of a neural network whose hidden layer is trained to be predictive of the classification task but uninformative as to the domain of the input our experiments on a sentiment analysis classification benchmark where the target domain data available at training time is unlabeled show that our neural network for domain adaption algorithm has better performance than either a standard neural network or an svm even if trained on input features extracted with the stateoftheart marginalized stacked denoising autoencoders of chen et al']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-ZoLdTpMBtg"
      },
      "source": [
        "## **Embeddings!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLwEZ5NzMA20"
      },
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module (Here we're making use of version 4)\r\n",
        "# This will take a while but won't be long :)\r\n",
        "\r\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"  \r\n",
        "appreciate = hub.load(module_url)\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MVzWpcONq7p"
      },
      "source": [
        "# REVIEW OUTPUT ::\r\n",
        "\r\n",
        "appreciate.variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFVpJJ-kOgJK"
      },
      "source": [
        "# Wrapping up with the U-S-E\r\n",
        "\r\n",
        "X_train = appreciate(X_train)\r\n",
        "X_test = appreciate(X_test)\r\n",
        "X_train = X_train.numpy()\r\n",
        "X_test = X_test.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDz7WwWLOzwh"
      },
      "source": [
        "# PREVIEW OUTPUT ::\r\n",
        "\r\n",
        "print(X_train[:10])\r\n",
        "print(y_train[:10])\r\n",
        "print(X_test[:10])\r\n",
        "print(y_test[:10])\r\n",
        "print(X_train.shape, X_test.shape, y_test.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk1zrMu1Q_QW"
      },
      "source": [
        "## **Building the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE2cmc3eQ9qk"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(512, input_shape=[512], activation = 'relu'))\r\n",
        "model.add(Dense(units=len(vocabulary), activation = 'softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2vwFvKMFhUC"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12j8kuJWUAKJ"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=512, shuffle=True, epochs=100, validation_data=(X_test, y_test), callbacks=[LambdaCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15xhCzDgDTkq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsP9-iI1DcfG"
      },
      "source": [
        "# Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAgXRupVEGeH"
      },
      "source": [
        "vocabulary = numpy.array(vocabulary)\r\n",
        "numpy.save('vocabulary.npy', vocabulary)\r\n",
        "model.save('arxiv_abstract_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vzq-1bH0oOv"
      },
      "source": [
        "# Validate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HjOSk44EYuB"
      },
      "source": [
        "## Restore the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkPS7--aEe76"
      },
      "source": [
        "pre_trained_model = tf.keras.models.load_model('arxiv_abstract_model')\r\n",
        "pre_trained_model.summary()\r\n",
        "\r\n",
        "vocabulary = np.load('vocabulary.npy')\r\n",
        "print(len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ijkaeSPEzvd"
      },
      "source": [
        "## Start the demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Jy3Hbi0nlK"
      },
      "source": [
        "# Create function to predict and show detailed output\r\n",
        "\r\n",
        "def next_word(model, collection=[], extent=1):\r\n",
        "\r\n",
        "  for item in collection:\r\n",
        "    text = item\r\n",
        "    for i in range(extent):\r\n",
        "      prediction = model.predict(x=appreciate([item]).numpy())\r\n",
        "      idx = np.argmax(prediction[-1])\r\n",
        "      item += ' ' + vocabulary[idx]\r\n",
        "      \r\n",
        "      print(text + ' --> ' + item + '\\nNEXT WORD: ' + item.split(' ')[-1] + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tZjEA0MX5pr"
      },
      "source": [
        "# Testing on a collection of words\r\n",
        "\r\n",
        "text_collection = ['this article improve', 'deep adversarial', 'a nonconvex', 'parallel']\r\n",
        "\r\n",
        "next_word(pre_trained_model, text_collection)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECE3g5jm-xI7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}